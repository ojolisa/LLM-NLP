This is a LLM using the transformer architecture with self defined attention heads with the help of PyTorch. 
This code is a companion to the research paper written by me on the Impact of Transformers in NLP Tasks.
